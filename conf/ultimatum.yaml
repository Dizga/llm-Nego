hydra:
  job:
    chdir: false

experiment:
  method: dond_run_train
  description: "Easy. They should be learning."
  nb_iterations: 400
  nb_matches_per_iteration: 10
  reinit_matches_each_it: true

matches:

  stop_condition: game_over_condition
  stop_condition_kwargs: {}

  run_matches_args:
    nb_parallel_matches: -1
    log_matches: true

  dond_game_args:
    rounds_per_game: 6
    max_turns: 1
    mode: basic
    random_setup_func: fixed_manual
    random_setup_kwargs: 
      items: ['coins']
      quantities: [10]
      val_starting_negotiator: [1]
      val_responding_negotiator: [3]
    role_assignator_func: alternating_role_assignator
    role_assignator_func_kwargs: {}
    finalization_visibility: True


  players: 

    alice:

      dond_player_args:
        mod_adpt_id: 'llama/ad_alice'
        allow_reasoning: false
        game_intro_file: "src/prompts/dond/rules.txt"
        in_between_file: "src/prompts/dond/between_moves.txt" 
        finalization_file: "src/prompts/dond/finalization.txt"
        new_round_file: "src/prompts/dond/new_round.txt"
        max_retries: 1

      ppo_data_extraction_args:

        last_k_responses: -1
        remove_errors: false

        score_function: points_score
        score_function_kwargs: 
          no_agreement_score: -1
          exponent: 1.0

        scores_pp_func: null
        scores_pp_func_kwargs: {}

        filter_out_func: null
        filter_out_func_kwargs: {}

    bob:

      dond_player_args:
        mod_adpt_id: 'llama/ad_bob'
        allow_reasoning: false
        game_intro_file: "src/prompts/dond/rules.txt"
        in_between_file: "src/prompts/dond/between_moves.txt" 
        finalization_file: "src/prompts/dond/finalization.txt"
        new_round_file: "src/prompts/dond/new_round.txt"
        max_retries: 1

      ppo_data_extraction_args:

        last_k_responses: -1
        remove_errors: false

        score_function: points_score
        score_function_kwargs: 
          no_agreement_score: -1
          exponent: 1.0

        scores_pp_func: null
        scores_pp_func_kwargs: {}

        filter_out_func: null
        filter_out_func_kwargs: {}

models: 

  llama:

    class: hf

    init_args:

      name: 'llama'
      adapter_names: ['sp_adapter', 'ad_alice', 'ad_bob']
      device: "cuda"
      default_training_mode: 'ppo'
      pretrained_args: 
        pretrained_model_name_or_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
        torch_dtype: "bfloat16"
        device_map: "auto"
      bits_and_bytes_args: null
        #load_in_8bit: False
        #load_in_4bit: true
      lora_args:
        task_type: TaskType.CAUSAL_LM
        r: 32
        lora_alpha: 16
        lora_dropout: 0.1
        target_modules: "all-linear"

      generation_args:
        max_new_tokens: 200
        do_sample: True
        temperature: 1.0
        top_k: 0.0
        top_p: 1.0
        repetition_penalty: 1.0
      keep_vllm_during_training: False
      keep_hf_during_generation: False
      destroy_ppo_trainer_after_training: True
      generate_with: "vllm"

    train_ppo_args:
      
      ppo_trainer_class: PPOTrainer
      step_batch_size: -1
      parallel_batch_size: 1

      ppo_trainer_args:
        batch_size: None
        mini_batch_size: None
        gradient_accumulation_steps: None
        ppo_epochs: 1
        #remove_unused_columns: false
        #vf_coef: 0.0
        gradient_checkpointing: False
        log_with: "tensorboard"
        project_kwargs: None
        tracker_project_name: "tensorboard"
        #target_kl: 0.1
        #cliprange: 0.2
        # adap_kl_ctrl: False
        # init_kl_coef: 0.0
        # target_kl: 5.0
        # learning_rate: 1e-5
        is_peft_model: True
      





