

device: "cuda"
hydra:
  job:
    chdir: false

cuda: true

playing:
  nb_iterations: 50
  games_per_iteration: 10

game:
  rounds_per_game: 1
  max_turns: 10
  mode: coop 
  setup: random_read
  setups_file: src/environments/dond_setups.txt

training:
  train_type: "ppo"
  checkpoint_models: False
  nb_epochs: 1 # Number of passes on entire dataset.
  ppo_trainer_args: # see https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_config.py
    # For batch size vs mini confusion https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py#L647)
    batch_size: 1 # Amount of examples loaded on the GPU. Must be divisable by mini_batch_size.
    mini_batch_size: 1 # Number of examples fed in parallel to the model before taking gradient step.
    ppo_epochs: 4 # Default: 4. Number of optimisation epochs per batch of samples
    model_name: "model"
    #learning_rate: 1.41e-5
    gradient_checkpointing: False # Whether to keep all activations in memory at once.
    gradient_accumulation_steps: 1 # Default: 1. Number of minibatch gradient step before accumulation
    log_with: "tensorboard"
    project_kwargs: None # don't touch, will be set later
    tracker_project_name: "tensorboard"


player_0:

  type: oai

  player_args:
    game_intro_file: "src/prompts/dereck/cooprules.txt"
    chain_of_thought_file: "src/prompts/basic/cot.txt" # false if none, else link to txt file 
    max_retries: 5
    finalization_file: "src/prompts/basic/finalization.txt"
    new_round_file: "src/prompts/coop/new_round.txt"


  agent_args:
    name: "openai_agent"
    api_key: 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
    model: gpt-4o-mini

player_1:

  type: "same_as_player_0"

  player_args: "same_as_player_0"

  agent_args: "same_as_player_0"

    






