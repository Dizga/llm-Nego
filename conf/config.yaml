device: "cuda"
hydra:
  job:
    chdir: false

cuda: true

playing:
  nb_iterations: 10
  games_per_iteration: 100

game:
  rounds_per_game: 1
  max_turns: 50
  mode: coop # coop
  setup: random_read
  setups_file: src/environments/dond_setups.txt


player_0:

  type: hf

  player_args:
    game_intro_file: "src/prompts/coop/rules.txt"
    chain_of_thought_file: "src/prompts/coop/cot.txt" # false if none, else link to txt file 
    max_retries: 2
    proposal_file: "src/prompts/coop/proposal.txt"

  agent_args:
    name: "agent_0"
    device: cuda
    tokenizer: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    inherit_model: False
    
    model_args: 
        pretrained_model_name_or_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
        torch_dtype: "auto"
        device_map: "auto"
        trust_remote_code: True

    lora_args:
        task_type: TaskType.CAUSAL_LM
        r: 16
        lora_alpha: 2
        lora_dropout: 0.1
        target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    model_training_args:
      output_dir: out_folder
      num_train_epochs: 1
      fp16: True
      per_device_train_batch_size: 3
      learning_rate: 5e-5
      weight_decay: 0.01
      logging_dir: os.path.join(out_folder, 'models', 'logs')
      logging_steps: 10
      save_total_limit: 2
      save_steps: 500
      evaluation_strategy: "steps"
      eval_steps: 500
      load_best_model_at_end: True

player_1:

  type: "same_as_player_0"

  player_args:
    game_intro_file: "same_as_player_0"
    chain_of_thought_file: "same_as_player_0"
    max_retries: "same_as_player_0"
    proposal_file: "same_as_player_0"

  agent_args:
    name: "agent_1"
    device: "same_as_player_0"
    tokenizer: "same_as_player_0"
    inherit_model: True
    model_args: "same_as_player_0"
    lora_args: "same_as_player_0"
    model_training_args: "same_as_player_0"
    
training:
  train_type: "ppo"
  nb_epochs: 1
  ppo_trainer_args:
    batch_size: "auto"
    mini_batch_size: 2
    model_name: "model"
    learning_rate: 1.41e-5
    gradient_checkpointing: False
    gradient_accumulation_steps: 1



