device: "cuda"
hydra:
  job:
    chdir: false

cuda: true

playing:
  nb_iterations: 50
  games_per_iteration: 2

game:
  rounds_per_game: 1
  max_turns: 10
  mode: semicomp # semicomp, coop
  setup: random_read
  setups_file: src/environments/dond_setups.txt

training:
  train_type: "ppo"
  checkpoint_models: False
  nb_epochs: 1 # Number of passes on entire dataset.
  ppo_trainer_args: # see https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_config.py
    # For batch size vs mini confusion https://github.com/huggingface/trl/blob/main/trl/trainer/ppo_trainer.py#L647)
    batch_size: 1 # Amount of examples loaded on the GPU. Must be divisable by mini_batch_size.
    mini_batch_size: 1 # Number of examples fed in parallel to the model before taking gradient step.
    ppo_epochs: 4 # Default: 4. Number of optimisation epochs per batch of samples
    model_name: "model"
    #learning_rate: 1.41e-5
    gradient_checkpointing: False # Whether to keep all activations in memory at once.
    gradient_accumulation_steps: 1 # Default: 1. Number of minibatch gradient step before accumulation
    log_with: "tensorboard"
    project_kwargs: None # don't touch, will be set later
    tracker_project_name: "tensorboard"


player_0:

  type: hf

  player_args:
    game_intro_file: "src/prompts/coop/rules.txt"
    chain_of_thought_file: "src/prompts/coop/cot.txt" # false if none, else link to txt file 
    max_retries: 2
    proposal_file: "src/prompts/coop/proposal.txt"
    new_round_file: "src/prompts/coop/new_round.txt"

  agent_args:
    name: "agent_0"
    device: cuda
    tokenizer_name: "meta-llama/Meta-Llama-3.1-8B-Instruct"
    inherit_model: False
    
    model_args: 
        pretrained_model_name_or_path: "meta-llama/Meta-Llama-3.1-8B-Instruct"
        torch_dtype: "auto"
        device_map: "auto"
        trust_remote_code: True

    bits_and_bytes_args:
        load_in_8bit: True

    lora_args:
        task_type: TaskType.CAUSAL_LM
        r: 16
        lora_alpha: 2
        lora_dropout: 0.1
        target_modules: ["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

    model_training_args:
      output_dir: out_folder
      num_train_epochs: 1
      fp16: True
      per_device_train_batch_size: 3
      learning_rate: 5e-5
      weight_decay: 0.01
      logging_dir: os.path.join(out_folder, 'models', 'logs')
      logging_steps: 10
      save_total_limit: 2
      save_steps: 500
      evaluation_strategy: "steps"
      eval_steps: 500
      load_best_model_at_end: True

player_1:

  type: "same_as_player_0"

  player_args: "same_as_player_0"

  agent_args: "same_as_player_0"

    






