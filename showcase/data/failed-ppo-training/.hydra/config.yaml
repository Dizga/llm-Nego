device: cuda
cuda: true
playing:
  nb_iterations: 10
  games_per_iteration: 20
game:
  rounds_per_game: 1
  max_turns: 10
  mode: basic
  setup: random_read
  setups_file: src/environments/dond_setups.txt
training:
  train_type: ppo
  checkpoint_models: false
  nb_epochs: 1
  ppo_trainer_args:
    batch_size: 1
    mini_batch_size: 1
    ppo_epochs: 4
    model_name: model
    gradient_checkpointing: false
    gradient_accumulation_steps: 1
    log_with: tensorboard
    project_kwargs: None
    tracker_project_name: tensorboard
player_0:
  type: hf
  player_args:
    game_intro_file: src/prompts/rules.txt
    chain_of_thought_file: src/prompts/cot.txt
    finalization_file: src/prompts/finalization.txt
    new_round_file: src/prompts/new_round.txt
    max_retries: 2
  agent_args:
    name: agent_0
    device: cuda
    tokenizer_name: meta-llama/Meta-Llama-3.1-8B-Instruct
    model_training_args:
      output_dir: out_folder
      num_train_epochs: 1
      fp16: true
      per_device_train_batch_size: 3
      learning_rate: 5.0e-05
      weight_decay: 0.01
      logging_dir: os.path.join(out_folder, 'models', 'logs')
      logging_steps: 10
      save_total_limit: 2
      save_steps: 500
      evaluation_strategy: steps
      eval_steps: 500
      load_best_model_at_end: true
  inherit_model: false
  model_args:
    pretrained_args:
      pretrained_model_name_or_path: meta-llama/Meta-Llama-3.1-8B-Instruct
      torch_dtype: bfloat16
      device_map: auto
    bits_and_bytes_args:
      load_in_4bit: false
    lora_args:
      task_type: TaskType.CAUSAL_LM
      r: 16
      lora_alpha: 2
      lora_dropout: 0.1
      target_modules:
      - q_proj
      - k_proj
      - v_proj
      - o_proj
      - gate_proj
      - up_proj
      - down_proj
player_1:
  type: same_as_player_0
  player_args: same_as_player_0
  agent_args: same_as_player_0
  inherit_model: true
  model_args: same_as_player_0
